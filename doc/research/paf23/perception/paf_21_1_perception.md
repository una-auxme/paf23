# Paf_21_1 - Perception

**Summary:** This page contains the research into the perception component of the PAF21_1 group.

- [Paf\_21\_1 - Perception](#paf_21_1---perception)
  - [1. Architecture](#1-architecture)
    - [**Key Features**](#key-features)
  - [2. Sensors](#2-sensors)
  - [3. Object-Detection](#3-object-detection)
  - [4. TrafficLight-Detection](#4-trafficlight-detection)
  - [5. Conclusion](#5-conclusion)

## 1. Architecture

![image](https://github.com/una-auxme/paf/assets/102369315/07328c78-83d7-425c-802e-8cc49430e6c1)

### **Key Features**

- Object-Detection
- TrafficLight-Detection

## 2. Sensors

- Position
- Velocity
- RGB-Image
- Depth-Image
- Semantic-Image

## 3. Object-Detection

- Objects are detected using semantic and depth images
- First they use masks and filters on the semantic image to identify pedestrians and vehicles
- Then they use the depth image and the position and velocity data to calculate the relative positions of each object

## 4. TrafficLight-Detection

- They use the semantic image and apply masks and filters to find traffic lights
- Then they use this information anlognside with the depth image to cut out a specific patch of the image
- After that they cut the same patch from the RGB Image (32x32x3) which is than classified by a simple Convolutional Neural Network (CNN)
- They use the classes Backside, Green, Yellow and Red

## 5. Conclusion

- Some parts of their systems, like the CNN, are probably usefull to us
- Since their entire perception is based on semantic and depth images it will be vewry hard for us to adopt their concepts
- We would have to build our own semantic and depth cameras which is a very complex task
- All in all the concepts of paf_21_1 regarding the perception arenÂ´t to usefull for our project
